{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw=pd.read_csv(\"raw_data/train.csv\", index_col=\"Id\")\n",
    "print(train_raw.shape)\n",
    "test_raw=pd.read_csv(\"raw_data/test.csv\", index_col=\"Id\")\n",
    "test_raw[\"SalePrice\"] = 0\n",
    "print(test_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_raw,test_raw], axis=0, sort=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = 'Numerical', 'Ordinal', 'Nomical'\n",
    "sizes = [27.5, 19, 53.5]\n",
    "explode = (0.1,0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Overall\n",
    "train_raw.hist(bins = 30, figsize = (20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_col(data):\n",
    "    coln = data.describe().columns\n",
    "    numeric_features=[]\n",
    "    for col in coln :\n",
    "        if len(data[col].value_counts()) >16 :\n",
    "            numeric_features.append(col)\n",
    "    return numeric_features\n",
    "\n",
    "numeric_features = numeric_col(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(train_raw[numeric_features], figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#box plot overallqual/saleprice\n",
    "var = \"OverallQual\"\n",
    "data = pd.concat([train_raw['SalePrice'], train_raw[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n",
    "fig.axis(ymin=0, ymax=800000);\n",
    "\n",
    "#sactter plot YearBuilt/saleprice\n",
    "var =  \"YearBuilt\" \n",
    "data = pd.concat([train_raw['SalePrice'], train_raw[var]], axis=1)\n",
    "data.plot.scatter(x=var, y = 'SalePrice', ylim=(0,800000),figsize=(8, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check Skewness of target value \n",
    "sns.distplot(train_raw['SalePrice'])\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train_raw['SalePrice'], plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =np.log1p(train_raw[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(x, fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(x, plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missingness  Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing value counter\n",
    "def na_counter(data):\n",
    "    coln = list(data.columns)\n",
    "    df_na ={}\n",
    "    for col in coln:\n",
    "        if data[col].isnull().any()==True:\n",
    "            mvalue = data[col].isnull().sum()\n",
    "            mpercent = data[col].isnull().sum()/len(data)\n",
    "            df_na[col]=[mvalue, mpercent]\n",
    "    df_Na =pd.DataFrame(df_na).T\n",
    "    df_Na.columns = ['NA',\"PERCENT\"]\n",
    "    return df_Na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_df = na_counter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_df[\"PERCENT\"].sort_values(ascending = False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation \n",
    "\n",
    "#### 1. Simply \"None\"\n",
    "\n",
    "BsmtQual: Evaluates the height of the basement\n",
    "\n",
    "       Ex\tExcellent (100+ inches)\t\n",
    "       Gd\tGood (90-99 inches)\n",
    "       TA\tTypical (80-89 inches)\n",
    "       Fa\tFair (70-79 inches)\n",
    "       Po\tPoor (<70 inches\n",
    "       NA\tNo Basement\n",
    "       \n",
    "#### 2. Information from other columns\n",
    "\n",
    "MSZoning: Identifies the general zoning classification of the sale.\n",
    "\n",
    "       A\tAgriculture\n",
    "       C\tCommercial\n",
    "       FV\tFloating Village Residential\n",
    "       I\tIndustrial\n",
    "       RH\tResidential High Density\n",
    "       RL\tResidential Low Density\n",
    "       RP\tResidential Low Density Park \n",
    "       RM\tResidential Medium Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoning - Neighborhood correlate\n",
    "test.loc[test[\"MSZoning\"].isnull(),['Neighborhood']]\n",
    "# check MSZoning values\n",
    "test.loc[test['Neighborhood']==\"IDOTRR\", \"MSZoning\"].value_counts()\n",
    "# Most of IDOTRR -> RM zone\n",
    "train_test[\"MSZoning\"] = train_test[\"MSZoning\"].fillna(\"RM\")\n",
    "\n",
    "\n",
    "#LotFrontage based on Neighborhood feature\n",
    "train_test[\"LotFrontage\"] = train_test[\"LotFrontage\"].fillna(df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\"mean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_fields(df):\n",
    "    \n",
    "    print(\"\\n\\n*****\\nImputing Null values....\")\n",
    "    print(\"Null counts before imputing:\")\n",
    "    print(na_counter(df))\n",
    "    \n",
    "\n",
    "    \"\"\" ?? \"\"\"\n",
    "    df[\"MSZoning\"] = df[\"MSZoning\"].fillna(\"RM\")\n",
    "    df[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\"mean\"))\n",
    "\n",
    "    df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(\"None\") \n",
    "    df[\"MasVnrArea\"] = df[\"MasVnrArea\"].fillna(0.0) \n",
    "    df[\"BsmtQual\"] = df[\"BsmtQual\"].fillna(\"NoBsmt\")\n",
    "    df[\"BsmtCond\"] = df[\"BsmtCond\"].fillna(\"NoBsmt\")\n",
    "    df[\"BsmtExposure\"] = df[\"BsmtExposure\"].fillna(\"NoBsmt\")\n",
    "    df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].fillna(\"NoBsmt\")\n",
    "    df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].fillna(\"NoBsmt\")\n",
    "    df['Exterior1st'] = df['Exterior1st'].fillna(\"None\") \n",
    "    df['Exterior2nd'] = df['Exterior2nd'].fillna(\"None\")\n",
    "    df['BsmtFinSF1'] = df['BsmtFinSF1'].fillna(0) \n",
    "    df['BsmtFinSF2'] = df['BsmtFinSF2'].fillna(0) \n",
    "    df['BsmtUnfSF'] = df['BsmtUnfSF'].fillna(0) \n",
    "    df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(0) \n",
    "    \n",
    "\n",
    "    \"\"\" Only one row missing the value, impute with \"SBrkr\" which is the most used \"\"\"\n",
    "    df.Electrical = df.Electrical.fillna(\"SBrkr\")\n",
    "\n",
    "    \"\"\" Impute with equavelant of overall house quality : Average, equivalent \"TA\" \"\"\"\n",
    "    df.KitchenQual = df.KitchenQual.fillna(\"None\")\n",
    "\n",
    "    df.FireplaceQu = df.FireplaceQu.fillna(\"None\")\n",
    "    \n",
    "    \"\"\" First change Garage Type for row with Null area and then change Garage Area \"\"\"\n",
    "    df.loc[df.GarageArea.isnull(), \"GarageType\"] = np.nan\n",
    "    df.loc[df.GarageArea.isnull(), \"GarageArea\"] = 0\n",
    "    \n",
    "        \n",
    "\n",
    "    \"\"\" For one record with Garage Area 360 and with missing Garage values, set to average values based on Overall Quality \"\"\"\n",
    "    df.loc[(df.GarageArea==360) & df.GarageFinish.isnull(), \"GarageFinish\"] = \"Unf\"\n",
    "    df.loc[(df.GarageArea==360) & df.GarageQual.isnull(), \"GarageQual\"] = \"TA\"\n",
    "    df.loc[(df.GarageArea==360) & df.GarageCond.isnull(), \"GarageCond\"] = \"TA\"\n",
    "    \"\"\" Setting the Garage built to \"Year Remodelled\" \"\"\"\n",
    "    df.loc[(df.GarageArea==360) & df.GarageYrBlt.isnull(), \"GarageYrBlt\"] = df.YearRemodAdd\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\" Set rest of the missing Garage valies to None and GarageCars to zero\"\"\"\n",
    "    df.GarageFinish = df.GarageFinish.fillna(\"None\")\n",
    "    df.GarageQual = df.GarageQual.fillna(\"None\")\n",
    "    df.GarageCond = df.GarageCond.fillna(\"None\")\n",
    "    df.GarageType = df.GarageType.fillna(\"None\")\n",
    "\n",
    "    df.GarageCars = df.GarageCars.fillna(0)\n",
    "\n",
    "\n",
    "    \"\"\" Is it correct to fill with Zero for Year built as the scale will change compared to most values in 1900\"\"\" \n",
    "    df.loc[df.GarageYrBlt.isnull(), \"GarageYrBlt\"] = df.YearRemodAdd\n",
    "\n",
    "\n",
    "    \"\"\" Impiute with Typ as mentioned in documentation. There does not seem to be clear relation with\n",
    "        Overall quality fields\n",
    "    \"\"\"\n",
    "    df.loc[df.Functional.isnull(), \"Functional\"] = \"Typ\"\n",
    "\n",
    "\n",
    "    \"\"\"Basement Fullbath and Halfbath NA values are related to  houses without basements. Soe set them to 0 \"\"\"\n",
    "    df.BsmtFullBath = df.BsmtFullBath.fillna(0)\n",
    "    df.BsmtHalfBath = df.BsmtHalfBath.fillna(0)\n",
    "    \n",
    "    \n",
    "    \"\"\" Other \"\"\"\n",
    "    df['SaleType'] = df['SaleType'].fillna(\"Oth\")\n",
    "    df['GarageQual'] = df['GarageQual'].fillna(\"None\")\n",
    "    \n",
    "    df[\"Alley\"] = df[\"Alley\"].fillna(\"None\")\n",
    "    df[\"Fence\"] = df[\"Fence\"].fillna(\"None\")\n",
    "    \n",
    "\n",
    "    print(\"\\n\\n\\nNull counts after imputing:\")\n",
    "    print(na_counter(df))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_data(df):\n",
    "    \n",
    "    print(\"\\n\\n*****\\nCorrecting incorrect data values...\")\n",
    "\n",
    "    \"\"\"Seems the year is 2207 is a typo, setting to the year remodeled\"\"\"\n",
    "    df.loc[df.GarageYrBlt==2207, \"GarageYrBlt\"] = 2007\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_ordinal(df):\n",
    "    typical_graded_cols = [\"HeatingQC\", \"KitchenQual\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"ExterQual\",\n",
    "                          \"ExterCond\", \"BsmtQual\", \"BsmtCond\"]\n",
    "    typical_grade_map = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0}\n",
    "\n",
    "    for i in typical_graded_cols:\n",
    "        df[i].replace(typical_grade_map, inplace=True)\n",
    "    \n",
    "    df['Functional'].replace({'Typ':7,'Min1':6,'Min2':5,'Mod':4,'Maj1':3,'Maj2':2,'Sev':1,'Sal':0}, inplace=True)\n",
    "    df['GarageFinish'].replace({'Fin':3,'RFn':2,'Unf':1,'None':0}, inplace=True)\n",
    "    df[\"BsmtExposure\"].replace({\"NoBsmt\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}, inplace=True)\n",
    "    df[\"GarageType\"].replace({\"attachd\" : 2, \"Detchd\": 1,\"BuiltIn\" :2, \"2Types\":0, \"CarPort\":0,'Basment':0}, inplace=True)\n",
    "    \n",
    "    df[\"MoSold\"].replace({1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0}, inplace =True)\n",
    "    df[\"MSSubClass\"].replace({20: 1, 30: 0, 40: 0, 45: 0,50: 0, 60: 1, 70: 0, 75: 0, 80: 0, 85: 0,\n",
    "         90: 0, 120: 1, 150: 0, 160: 0, 180: 0, 190: 0}, inplace =True)\n",
    "    \n",
    "    bsmt_fin_dict = {\"NoBsmt\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "    df[\"BsmtFinType1\"].map(bsmt_fin_dict).astype(int)\n",
    "    df[\"BsmtFinType2\"].map(bsmt_fin_dict).astype(int)\n",
    "    \n",
    "    df[\"Fence\"].replace({\"None\": 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transformation(df):\n",
    "    \n",
    "    print(\"\\n\\n*****\\n Transforming features ...\")\n",
    "\n",
    "\n",
    "    \"\"\" Changing Columns to Binary where majority values are one specific value and rest all had \n",
    "    similar pattern against SalePrice\n",
    "    \"\"\"\n",
    "    df[\"Electrical\"] = np.where(df[\"Electrical\"].str.contains(\"SBrkr\"),1, 0).astype(int)\n",
    "    df[\"Heating\"] = np.where(df[\"Heating\"].str.contains(\"GasA\"), 1, 0).astype(int)\n",
    "    df[\"RoofMatl\"] = np.where(df[\"RoofMatl\"].str.contains(\"CompShg|WdShngl\", regex=True), 1,0).astype(int)\n",
    "    df[\"MiscFeature\"] = np.where(df[\"MiscFeature\"].str.contains(\"Shed\"),1, 0).astype(int)\n",
    "    df[\"Condition2\"] = np.where(df[\"Condition2\"].str.contains(\"Norm\"),1, 0).astype(int)\n",
    "    \n",
    "\n",
    "    df[\"PoolArea\"]=np.where(df[\"PoolArea\"]>1,1,0)\n",
    "    df[\"WoodDeckSF\"]=np.where(df[\"WoodDeckSF\"]>1,1,0)\n",
    "    df[\"OpenPorchSF\"]=np.where(df[\"OpenPorchSF\"]>1,1,0)\n",
    "    df[\"EnclosedPorch\"]=np.where(df[\"EnclosedPorch\"]>1,1,0)\n",
    "    df[\"3SsnPorch\"]=np.where(df[\"3SsnPorch\"]>1,1,0)\n",
    "    df[\"ScreenPorch\"]=np.where(df[\"ScreenPorch\"]>1,1,0)\n",
    "    df[\"LowQualFinSF\"]=np.where(df[\"LowQualFinSF\"]>1,0,1)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation(df):\n",
    "    \n",
    "    print(\"\\n\\n*****\\n Transforming data values ...\")\n",
    "\n",
    "    df.loc[(df.OverallCond==2) &( df.SalePrice>300000), \"OverallCond\"] = 5\n",
    "    df[\"LotFrontage\"][df[\"LotFrontage\"]>150]= 174\n",
    "    df.loc[(df.LotArea>100000), \"LotArea\"] = 70000\n",
    "    df.loc[(df.BsmtFinSF1>=3000),\"BsmtFinSF1\"]=4000\n",
    "    \n",
    "    # train_raw[\"SalePrice\"].groupby(train_raw[\"Neighborhood\"]).mean().sort_values()\n",
    "\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(df):\n",
    "    \n",
    "    \"\"\" remodelling year\"\"\" \n",
    "    df[\"Remod\"] = (df[\"YearRemodAdd\"] != df[\"YearBuilt\"]) * 1\n",
    "    \n",
    "    \"\"\" Brand New House \"\"\"\n",
    "    df[\"BNewHouse\"] = (df[\"YearBuilt\"] == df[\"YearBuilt\"].max()) * 1\n",
    "    \n",
    "    \"\"\"\"Age of building \"\"\" \n",
    "    df[\"Age\"] = df[\"YearBuilt\"].max() - df[\"YearBuilt\"]\n",
    "   \n",
    "    \"\"\"\" Total Area\"\"\"\n",
    "    area = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "                 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n",
    "                 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'LowQualFinSF', 'PoolArea' ]\n",
    "    df[\"TotalArea\"] = df[area].sum(axis=1)\n",
    "    \n",
    "    \"\"\" Consolidate columns and drop related columns\"\"\"\n",
    "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "    df[\"TotalFullBaths\"] = df[\"BsmtFullBath\"]+df[\"FullBath\"]\n",
    "    df = df.drop([\"BsmtFullBath\",\"FullBath\", \"BsmtHalfBath\", \"HalfBath\" ], axis=1)\n",
    "    \n",
    "    \"\"\" Dropping 1st and 2nd Floor SFT as they are collinear with GrLivArea\"\"\"\n",
    "    df = df.drop([ '1stFlrSF', '2ndFlrSF',], axis=1)\n",
    "    \n",
    "    \"\"\" Combine Porchs\"\"\"\n",
    "    df[\"TotalPorch\"] = df[\"OpenPorchSF\"]+df[\"EnclosedPorch\"]+df[\"3SsnPorch\"]+df[\"ScreenPorch\"]\n",
    "    df = df.drop([ \"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"], axis=1)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    \n",
    "    print(\"\\n\\n*****\\nDropping selective columns...\")\n",
    "    columns_to_drop = [\"Street\",\"Utilities\",\"PoolQC\", \"MiscVal\"]\n",
    "\n",
    "    columns_to_drop = columns_to_drop + [\"BsmtFinSF2\"]\n",
    "\n",
    "    \n",
    "    print(\"\\n\\n*****\\nNo of columns before dropping : \", len(df.columns))\n",
    "    print(\"No of columns to drop : \", len(columns_to_drop))\n",
    "    \n",
    "    df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    print(\"No of columns after dropping : \", len(df.columns))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = impute_null_fields(df)\n",
    "df = correct_data(df)\n",
    "df = transform_ordinal(df)\n",
    "df = feature_transformation(df)\n",
    "df = data_transformation(df)\n",
    "df = feature_generation(df)\n",
    "df = drop_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"train_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"train_test.csv\", index_col=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features  = df.describe().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "scores = {}\n",
    "lm = linear_model.LinearRegression()\n",
    "from sklearn.metrics import r2_score\n",
    "for feature_name in numeric_features:\n",
    "    df_numeric = df[numeric_features].copy()\n",
    "    feature = df[feature_name].copy()\n",
    "    df_numeric.drop(feature_name, axis=1, inplace=True)\n",
    "    lm.fit(df_numeric, feature)\n",
    "    scores[feature_name] = lm.score(df_numeric, feature)  \n",
    "    \n",
    "sns.barplot(x='index', y='R2', data=pd.DataFrame(scores, index=['R2']).T.reset_index())\n",
    "plt.title('$R^2$ of a continuous feature against the other features')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['TotalSF'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the skew of all numerical features\n",
    "df_log = df.copy()\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew \n",
    "\n",
    "def numeric_col(data):\n",
    "    coln = data.describe().columns\n",
    "    numeric_features=[]\n",
    "    for col in coln :\n",
    "        if len(data[col].value_counts()) >16 :\n",
    "            numeric_features.append(col)\n",
    "    return numeric_features\n",
    "\n",
    "numeric_features = numeric_col(df)\n",
    "\n",
    "skewed_feats =df_log[numeric_features].apply(lambda x: skew(x.dropna().astype(float))).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "print(skewness)\n",
    "skewness = skewness[abs(skewness)>0.75]\n",
    "df_log[skewness.index] = np.log1p(df_log[skewness.index])\n",
    "\n",
    "# Additional processing: scale the data.   \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_log[numeric_features])\n",
    "\n",
    "scaled = scaler.transform(df_log[numeric_features])\n",
    "for i, col in enumerate(numeric_features):\n",
    "    df_log[col] = scaled[:, i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skewness.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#boxcox\n",
    "df_bc =df.copy()\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "skewed_feats =df_bc[numeric_features].apply(lambda x: skew(x.dropna().astype(float))).sort_values(ascending=False)\n",
    "df_bc[skewness.index] = boxcox1p(df_bc[skewness.index], 0.15)\n",
    "\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_bc[numeric_features])\n",
    "\n",
    "scaled = scaler.transform(df_bc[numeric_features])\n",
    "for i, col in enumerate(numeric_features):\n",
    "    df_bc[col] = scaled[:, i]\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_log.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def value_counts(data):        \n",
    "    for col in (data.columns):\n",
    "        if len(data[col].value_counts()) <26 :\n",
    "            print(data[col].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dum = df_log.copy()\n",
    "df_bc_dum = df_bc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_dummies(df):\n",
    "    cat_col=[]\n",
    "    coln = df.columns\n",
    "    for col in coln :\n",
    "        if len(df[col].value_counts()) <26 :\n",
    "            cat_col.append(col)\n",
    "    for feature in cat_col:\n",
    "        new_dummy = pd.get_dummies(df[feature], prefix=feature,drop_first=False)\n",
    "        df = pd.concat([df.drop(feature, axis=1), new_dummy], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dum =make_dummies(df_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dum = df_dum.drop(['MSSubClass_0', 'MSZoning_C (all)',\n",
    "                     \"Alley_Pave\", 'LotShape_IR3',\n",
    "                    'LandContour_Low','LotConfig_FR3', 'LandSlope_Sev', 'Neighborhood_Blueste', \n",
    "                    'Condition1_RRNe',\"Condition2_0\",\n",
    "                    'BldgType_2fmCon', 'HouseStyle_2.5Fin', 'OverallQual_1', 'OverallCond_1',\"RoofStyle_Shed\",\n",
    "                    \"RoofMatl_0\",\"Exterior1st_None\",\"Exterior2nd_None\",\"Exterior2nd_Other\",\"MasVnrType_BrkCmn\",\n",
    "                    \"ExterQual_2\",\"ExterCond_1\",\"Foundation_Wood\",\"BsmtQual_2\",\"BsmtCond_1\",\"BsmtExposure_0\",\n",
    "                    \"BsmtFinType2_GLQ\",\"Heating_0\",\"HeatingQC_1\",\"Electrical_0\",\n",
    "                    \"LowQualFinSF_0\",\"BedroomAbvGr_8\",\"KitchenAbvGr_3\",\"KitchenQual_0\",\"TotRmsAbvGrd_2\",\"TotRmsAbvGrd_14\",\n",
    "                    \"TotRmsAbvGrd_13\",\"TotRmsAbvGrd_15\",\"Functional_1\",\"Fireplaces_4\",\"FireplaceQu_5\",\"GarageType_0\",\n",
    "                    \"GarageFinish_0\",\"GarageCars_5.0\",\"GarageQual_5\",\"GarageCond_5\",\"PavedDrive_P\",\"WoodDeckSF_0\",\n",
    "                    \"PoolArea_0\",\"Fence_MnWw\",\"YrSold_2010\",\"SaleType_Con\",\"SaleCondition_AdjLand\",\"Remod_0\",\"BNewHouse_1\",\n",
    "                    \"TotalFullBaths_0.0\",\"TotalPorch_3\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc_dum =make_dummies(df_bc_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc_dum = df_bc_dum.drop(['MSSubClass_0', 'MSZoning_C (all)',\n",
    "                     \"Alley_Pave\", 'LotShape_IR3',\n",
    "                    'LandContour_Low','LotConfig_FR3', 'LandSlope_Sev', 'Neighborhood_Blueste', \n",
    "                    'Condition1_RRNe',\"Condition2_0\",\n",
    "                    'BldgType_2fmCon', 'HouseStyle_2.5Fin', 'OverallQual_1', 'OverallCond_1',\"RoofStyle_Shed\",\n",
    "                    \"RoofMatl_0\",\"Exterior1st_None\",\"Exterior2nd_None\",\"Exterior2nd_Other\",\"MasVnrType_BrkCmn\",\n",
    "                    \"ExterQual_2\",\"ExterCond_1\",\"Foundation_Wood\",\"BsmtQual_2\",\"BsmtCond_1\",\"BsmtExposure_0\",\n",
    "                    \"BsmtFinType2_GLQ\",\"Heating_0\",\"HeatingQC_1\",\"Electrical_0\",\n",
    "                    \"LowQualFinSF_0\",\"BedroomAbvGr_8\",\"KitchenAbvGr_3\",\"KitchenQual_0\",\"TotRmsAbvGrd_2\",\"TotRmsAbvGrd_14\",\n",
    "                    \"TotRmsAbvGrd_13\",\"TotRmsAbvGrd_15\",\"Functional_1\",\"Fireplaces_4\",\"FireplaceQu_5\",\"GarageType_0\",\n",
    "                    \"GarageFinish_0\",\"GarageCars_5.0\",\"GarageQual_5\",\"GarageCond_5\",\"PavedDrive_P\",\"WoodDeckSF_0\",\n",
    "                    \"PoolArea_0\",\"Fence_MnWw\",\"YrSold_2010\",\"SaleType_Con\",\"SaleCondition_AdjLand\",\"Remod_0\",\"BNewHouse_1\",\n",
    "                    \"TotalFullBaths_0.0\",\"TotalPorch_3\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(df_bc, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Split data in 2 \n",
    "train, test = np.split(df_dum,[1460])\n",
    "test = test.drop(\"SalePrice\", axis=1)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "X = train.drop(\"SalePrice\", axis=1)\n",
    "y = np.log1p(train_raw[\"SalePrice\"])\n",
    "print(\"X and y:\" ,X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)\n",
    "# train_X, test_X, train_Y, test_Y = train_test_split(train_data.iloc[:,:-1], train_data.iloc[:,-1:],test_size=0.2, random_state=99)\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 10\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a rmsle evaluation function\n",
    "\n",
    "def rmsle(y, y_pred): \n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "ridge = linear_model.Ridge\n",
    "lasso = linear_model.Lasso\n",
    "ENet = linear_model.ElasticNet(alpha = 1, l1_ratio = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm.fit(X, y)\n",
    "lm_train_pred = lm.predict(X)\n",
    "lm_test_pred = np.expm1(lm.predict(test.values))\n",
    "print(rmsle(y, lm_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train)\n",
    "print(\"train: {:.2f}\".format(lm.score(X_train, y_train)))\n",
    "print(\"test: {:.2f}\".format(lm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(lm)\n",
    "print(\"\\nlm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge05 = ridge(alpha=10).fit(X_train, y_train)\n",
    "ridge05.fit(X, y)\n",
    "ridge05_train_pred = ridge05.predict(X)\n",
    "ridge05_test_pred = np.expm1(ridge05.predict(test))\n",
    "print(rmsle(y, ridge05_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = rmsle_cv(ridge05)\n",
    "print(\"train: {:.2f}\".format(ridge05.score(X_train, y_train)))\n",
    "print(\"test: {:.2f}\".format(ridge05.score(X_test, y_test)))\n",
    "print(\"\\nridge rmse score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lasso01 = Lasso(alpha=0.0003).fit(X, y)\n",
    "lasso01_train_pred = lasso01.predict(X)\n",
    "lasso01_test_pred = np.expm1(lasso01.predict(test))\n",
    "print(rmsle(y, lasso01_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.0003,max_iter=50000).fit(X_train, y_train)\n",
    "print(\"train: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"test: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"feature size: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nlasso rmse score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0003, l1_ratio=.9, random_state=3))\n",
    "EN.fit(X,y)\n",
    "ENet_train_pred = EN.predict(X)\n",
    "ENet_pred = np.expm1(EN.predict(test))\n",
    "print(rmsle(y, ENet_train_pred))\n",
    "EN.fit(X_train,y_train)\n",
    "print(\"train: {:.2f}\".format(EN.score(X_train, y_train)))\n",
    "print(\"test: {:.2f}\".format(EN.score(X_test, y_test)))\n",
    "score = rmsle_cv(EN)\n",
    "print(\"ElasticNet rmse score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_depth=9, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"train accuracy: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"test accuracy: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "score = rmsle_cv(tree)\n",
    "print(\"\\ntree rmsescore: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"feature importance:\\n{}\".format(tree.feature_importances_))\n",
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), X_train.columns)\n",
    "    plt.xlabel(\"importance\")\n",
    "    plt.ylabel(\"features\")\n",
    "    plt.ylim(-1, n_features)\n",
    "\n",
    "plot_feature_importances(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(n_estimators=3000, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"train acc: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"test acc: {:.3f}\".format(forest.score(X_test, y_test)))\n",
    "score = rmsle_cv(forest)\n",
    "print(\"\\nforest rmsescore: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBoost = GradientBoostingRegressor(random_state =0)\n",
    "# GBoost.fit(X_train, y_train)\n",
    "\n",
    "# print(\"GBoost acc: {:.3f}\".format(GBoost.score(X_train, y_train)))\n",
    "# print(\"GBoost acc: {:.3f}\".format(GBoost.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3500, learning_rate=0.05,\n",
    "                                   max_depth=3, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "GBoost.fit(X_train, y_train)\n",
    "print(\"GBoost train acc: {:.3f}\".format(GBoost.score(X_train, y_train)))\n",
    "print(\"GBoost test acc: {:.3f}\".format(GBoost.score(X_test, y_test)))\n",
    "score = rmsle_cv(GBoost)\n",
    "print(\"GBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost.fit(X,y)\n",
    "GBoost_train_pred = GBoost.predict(X)\n",
    "GBoost_pred = np.expm1(GBoost.predict(test))\n",
    "print(rmsle(y, GBoost_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# model_xgb = xgb.XGBRegressor(random_state =0)\n",
    "# score = rmsle_cv(model_xgb)\n",
    "# print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_xgb_c = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=7500,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =8, nthread = -1)\n",
    "\n",
    "score = rmsle_cv(model_xgb_c)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "model_xgb_c.fit(X_train, y_train)\n",
    "\n",
    "print(\"xgb train: {:.3f}\".format(model_xgb_c.score(X_train, y_train)))\n",
    "print(\"xgb test: {:.3f}\".format(model_xgb_c.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_c.fit(X,y)\n",
    "xgb_train_pred = model_xgb_c.predict(X)\n",
    "xgb_pred = np.expm1(model_xgb_c.predict(test))\n",
    "print(rmsle(y, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb = lgb.LGBMRegressor(objective='regression')\n",
    "# score = rmsle_cv(model_lgb)\n",
    "# print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb_c = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=2500,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(model_lgb_c)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n",
    "model_lgb_c.fit(X, y)\n",
    "lgb_train_pred = model_lgb_c.predict(X)\n",
    "lgb_pred = np.expm1(model_lgb_c.predict(test))\n",
    "print(rmsle(y, lgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "print(\"train acc: {:.2f}\".format(svr.score(X_train, y_train)))\n",
    "print(\"test acc: {:.2f}\".format(svr.score(X_test, y_test)))\n",
    "# svr = SVR(C=1000)\n",
    "# svr.fit(X_train, y_train)\n",
    "\n",
    "# print(\"train acc: {:.2f}\".format(svr.score(X_train, y_train)))\n",
    "# print(\"test acc: {:.2f}\".format(svr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_01 = SVR(C=1, cache_size=200, coef0=0, degree=3, epsilon=0.0, gamma='auto',\n",
    "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "svr_01.fit(X_train, y_train)\n",
    "\n",
    "print(\"train acc: {:.2f}\".format(svr_01.score(X_train, y_train)))\n",
    "print(\"test acc: {:.2f}\".format(svr_01.score(X_test, y_test)))\n",
    "score = rmsle_cv(svr_01)\n",
    "print(\"svc score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_01.fit(X, y)\n",
    "svr_01_train_pred = svr_01.predict(X)\n",
    "svr_01_test_pred = np.expm1(svr_01.predict(test))\n",
    "print(rmsle(y, svr_01_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = xgb_pred*0.25 + lgb_pred*0.5 + GBoost_pred*0.25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ENet_pred*0.5 + GBoost_pred*0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ensemble\n",
    "\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"SalePrice\"] = predictions\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"housemodel_ens_2gb2lgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "# We take the log here because the error metric is between the log of the\n",
    "# SalePrice and the log of the predicted price. That does mean we need to \n",
    "# exp() the prediction to get an actual sale price.\n",
    "label_df = pd.DataFrame(index = train_df_munged.index, columns=[\"SalePrice\"])\n",
    "label_df[\"SalePrice\"] = np.log(train_df[\"SalePrice\"])\n",
    "\n",
    "print(\"Training set size:\", train_df_munged.shape)\n",
    "print(\"Test set size:\", test_df_munged.shape)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# XGBoost -- I did some \"manual\" cross-validation here but should really find\n",
    "# these hyperparameters using CV. ;-)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "regr = xgb.XGBRegressor(\n",
    "                 colsample_bytree=0.2,\n",
    "                 gamma=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=4,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=7200,                                                                  \n",
    "                 reg_alpha=0.9,\n",
    "                 reg_lambda=0.6,\n",
    "                 subsample=0.2,\n",
    "                 seed=42,\n",
    "                 silent=1)\n",
    "\n",
    "regr.fit(train_df_munged, label_df)\n",
    "\n",
    "# Run prediction on training set to get a rough idea of how well it does.\n",
    "y_pred = regr.predict(train_df_munged)\n",
    "y_test = label_df\n",
    "print(\"XGBoost score on training set: \", rmse(y_test, y_pred))\n",
    "\n",
    "# Run prediction on the Kaggle test set.\n",
    "y_pred_xgb = regr.predict(test_df_munged)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# I found this best alpha through cross-validation.\n",
    "best_alpha = 0.00099\n",
    "\n",
    "regr = Lasso(alpha=best_alpha, max_iter=50000)\n",
    "regr.fit(train_df_munged, label_df)\n",
    "\n",
    "# Run prediction on training set to get a rough idea of how well it does.\n",
    "y_pred = regr.predict(train_df_munged)\n",
    "y_test = label_df\n",
    "print(\"Lasso score on training set: \", rmse(y_test, y_pred))\n",
    "\n",
    "# Run prediction on the Kaggle test set.\n",
    "y_pred_lasso = regr.predict(test_df_munged)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Blend the results of the two regressors and save the prediction to a CSV file.\n",
    "\n",
    "y_pred = (y_pred_xgb + y_pred_lasso) / 2\n",
    "y_pred = np.exp(y_pred)\n",
    "\n",
    "pred_df = pd.DataFrame(y_pred, index=test_df[\"Id\"], columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('ens_EN_xgb.csv', header=True, index_label='Id')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
